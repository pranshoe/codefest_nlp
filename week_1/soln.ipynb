{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b44445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg!!! i luv nlp <3. it's sooooo coool! #ai #machinelearning.\n",
      "btw, nlp isn't easy at all... but it's fun!!! ðŸ˜„ðŸ˜„\n",
      "in college, students read blogs, tweets, research papers, emails, and chat messages daily!!!\n",
      "some texts are clean; others are full of typos, slangggg, emojis ðŸ˜‚ðŸ˜‚, and random caps.\n",
      "in 2025, ai-powered systems process millions of documents @ scaleâ€”fast & continuously.\n",
      "if your data isn't clean and normalized properly, your model will fail badly!!! ðŸ’¥ðŸ’¥\n",
      "trust me: garbage in = garbage out.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"OMG!!! I luv NLP <3. It's sooooo coool! #AI #MachineLearning.\n",
    "BTW, NLP isn't easy at all... but it's FUN!!! ðŸ˜„ðŸ˜„\n",
    "In college, students read blogs, tweets, research papers, emails, and chat messages daily!!!\n",
    "Some texts are clean; others are FULL of typos, slangggg, emojis ðŸ˜‚ðŸ˜‚, and random CAPS.\n",
    "In 2025, AI-powered systems process MILLIONS of documents @ scaleâ€”fast & continuously.\n",
    "If your data isn't clean and normalized properly, your model WILL fail badly!!! ðŸ’¥ðŸ’¥\n",
    "Trust me: garbage in = garbage out.\"\"\".lower()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1193ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg i luv nlp <3 its sooooo coool ai machinelearning\n",
      "btw nlp isnt easy at all but its fun \n",
      "in college students read blogs tweets research papers emails and chat messages daily\n",
      "some texts are clean others are full of typos slangggg emojis  and random caps\n",
      "in 2025 aipowered systems process millions of documents  scalefast  continuously\n",
      "if your data isnt clean and normalized properly your model will fail badly \n",
      "trust me garbage in = garbage out\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub(r'[^\\w\\s<=]', '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0521dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omg', 'i', 'luv', 'nlp', '<', '3', 'its', 'sooooo', 'coool', 'ai', 'machinelearning', 'btw', 'nlp', 'isnt', 'easy', 'at', 'all', 'but', 'its', 'fun', 'in', 'college', 'students', 'read', 'blogs', 'tweets', 'research', 'papers', 'emails', 'and', 'chat', 'messages', 'daily', 'some', 'texts', 'are', 'clean', 'others', 'are', 'full', 'of', 'typos', 'slangggg', 'emojis', 'and', 'random', 'caps', 'in', '2025', 'aipowered', 'systems', 'process', 'millions', 'of', 'documents', 'scalefast', 'continuously', 'if', 'your', 'data', 'isnt', 'clean', 'and', 'normalized', 'properly', 'your', 'model', 'will', 'fail', 'badly', 'trust', 'me', 'garbage', 'in', '=', 'garbage', 'out']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f417e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omg', 'i', 'luv', 'nlp', '<', '3', 'it', 'sooooo', 'coool', 'ai', 'machinelearning', 'btw', 'nlp', 'isnt', 'easy', 'at', 'all', 'but', 'it', 'fun', 'in', 'college', 'student', 'read', 'blog', 'tweet', 'research', 'paper', 'email', 'and', 'chat', 'message', 'daily', 'some', 'text', 'are', 'clean', 'others', 'are', 'full', 'of', 'typo', 'slangggg', 'emojis', 'and', 'random', 'cap', 'in', '2025', 'aipowered', 'system', 'process', 'million', 'of', 'document', 'scalefast', 'continuously', 'if', 'your', 'data', 'isnt', 'clean', 'and', 'normalized', 'properly', 'your', 'model', 'will', 'fail', 'badly', 'trust', 'me', 'garbage', 'in', '=', 'garbage', 'out']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5a0305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'omg': 1, 'i': 1, 'luv': 1, 'nlp': 2, '<': 1, '3': 1, 'it': 2, 'sooooo': 1, 'coool': 1, 'ai': 1, 'machinelearning': 1, 'btw': 1, 'isnt': 2, 'easy': 1, 'at': 1, 'all': 1, 'but': 1, 'fun': 1, 'in': 3, 'college': 1, 'student': 1, 'read': 1, 'blog': 1, 'tweet': 1, 'research': 1, 'paper': 1, 'email': 1, 'and': 3, 'chat': 1, 'message': 1, 'daily': 1, 'some': 1, 'text': 1, 'are': 2, 'clean': 2, 'others': 1, 'full': 1, 'of': 2, 'typo': 1, 'slangggg': 1, 'emojis': 1, 'random': 1, 'cap': 1, '2025': 1, 'aipowered': 1, 'system': 1, 'process': 1, 'million': 1, 'document': 1, 'scalefast': 1, 'continuously': 1, 'if': 1, 'your': 2, 'data': 1, 'normalized': 1, 'properly': 1, 'model': 1, 'will': 1, 'fail': 1, 'badly': 1, 'trust': 1, 'me': 1, 'garbage': 2, '=': 1, 'out': 1}\n"
     ]
    }
   ],
   "source": [
    "vectors_preview = {}\n",
    "for word in tokens:\n",
    "    if word in vectors_preview:\n",
    "        vectors_preview[word]+=1\n",
    "    else:\n",
    "        vectors_preview[word]=1\n",
    "\n",
    "print(vectors_preview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
